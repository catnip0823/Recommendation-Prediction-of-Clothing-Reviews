{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of cloth_CNN.ipynb","provenance":[{"file_id":"16MUYmmkvd3Hls2u6DoBaDy9per3E9LHj","timestamp":1638922941997},{"file_id":"1MU41kp6L3Eh5R-ck9EtfLXn0wyqlKBFI","timestamp":1638816093647},{"file_id":"1-4G-icZDEBTfsxoAU8yjRP7bOAB8kRCa","timestamp":1638236968316}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9oQwdYbpJCe","executionInfo":{"status":"ok","timestamp":1638816183535,"user_tz":300,"elapsed":14278,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"cbb313cc-7e8e-40b5-ff13-a4ed583cb89a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","# !wget https://github.com/explosion/spacy-models/releases/download/en_vectors_web_lg-2.1.0/en_vectors_web_lg-2.1.0.tar.gz -O en_vectors_web_lg-2.1.0.tar.gz\n","# !pip install en_vectors_web_lg-2.1.0.tar.gz\n","data_dir = '/content/gdrive/Shareddrives/520_Project'\n","# !pip install /content/gdrive/Shareddrives/520_Project/en_vectors_web_lg-2.1.0.tar.gz"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"NLpErMAE2VO7"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import torch\n","import re\n","import time\n","import copy\n","import math\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim as optimizer\n","import torch.nn.functional as F\n","from torch import nn\n","from sklearn.metrics import accuracy_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","# import en_vectors_web_lg\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","plt.rcParams['font.family'] = \"sans-serif\"\n","plt.rcParams['font.sans-serif'] = ['Times New Roman']\n","sns.set_style(\"whitegrid\")\n","sns.set_style({'font.family':'serif', 'font.serif':'Times New Roman'})\n","sns.set(font_scale=1.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"ALRfWp_JpLss","executionInfo":{"status":"ok","timestamp":1638816191124,"user_tz":300,"elapsed":1044,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"4ba67ce9-5748-462e-d971-028c8ae0e627"},"source":["data = pd.read_csv(os.path.join(data_dir, \"Womens_Clothing_E-Commerce_Reviews.csv\"))\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Clothing ID</th>\n","      <th>Age</th>\n","      <th>Title</th>\n","      <th>Review Text</th>\n","      <th>Rating</th>\n","      <th>Recommended IND</th>\n","      <th>Positive Feedback Count</th>\n","      <th>Division Name</th>\n","      <th>Department Name</th>\n","      <th>Class Name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>767</td>\n","      <td>33</td>\n","      <td>NaN</td>\n","      <td>Absolutely wonderful - silky and sexy and comf...</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Initmates</td>\n","      <td>Intimate</td>\n","      <td>Intimates</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1080</td>\n","      <td>34</td>\n","      <td>NaN</td>\n","      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>General</td>\n","      <td>Dresses</td>\n","      <td>Dresses</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1077</td>\n","      <td>60</td>\n","      <td>Some major design flaws</td>\n","      <td>I had such high hopes for this dress and reall...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>General</td>\n","      <td>Dresses</td>\n","      <td>Dresses</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1049</td>\n","      <td>50</td>\n","      <td>My favorite buy!</td>\n","      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>General Petite</td>\n","      <td>Bottoms</td>\n","      <td>Pants</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>847</td>\n","      <td>47</td>\n","      <td>Flattering shirt</td>\n","      <td>This shirt is very flattering to all due to th...</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>General</td>\n","      <td>Tops</td>\n","      <td>Blouses</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  Clothing ID  Age  ...   Division Name Department Name  Class Name\n","0           0          767   33  ...       Initmates        Intimate   Intimates\n","1           1         1080   34  ...         General         Dresses     Dresses\n","2           2         1077   60  ...         General         Dresses     Dresses\n","3           3         1049   50  ...  General Petite         Bottoms       Pants\n","4           4          847   47  ...         General            Tops     Blouses\n","\n","[5 rows x 11 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"I1MZosu64FTu"},"source":["def clean_text(w):\n","    return re.sub(\n","            r\"([.,'!?\\\"()*#:;])\",\n","            '',\n","            w.lower()\n","            ).replace('-', ' ').replace('/', ' ')\n","\n","def get_glove_embedding(reviews, data_dir):\n","  token_file = os.path.join(data_dir,'token_to_ix.pkl')\n","  glove_file = os.path.join(data_dir,'train_glove.npy')\n","  if os.path.exists(glove_file) and os.path.exists(token_file):\n","        print(\"Loading saved embedding\")\n","        return pickle.load(open(token_file, \"rb\")), np.load(glove_file)\n","  all_reviews = {}\n","  for idx, s in enumerate(reviews):\n","    all_reviews[idx] = clean_text(s).split()\n","\n","  from collections import defaultdict\n","  token_to_ix = defaultdict(int)\n","  token_to_ix['UNK'] = 1\n","\n","  spacy_tool = en_vectors_web_lg.load()\n","  pretrained_emb = []\n","  pretrained_emb.append(spacy_tool('UNK').vector)\n","  \n","  for k, v in all_reviews.items():\n","      for word in v:\n","          if word not in token_to_ix:\n","              token_to_ix[word] = len(token_to_ix)\n","              pretrained_emb.append(spacy_tool(word).vector)\n","\n","  pretrained_emb = np.array(pretrained_emb)\n","  np.save(glove_file, pretrained_emb)\n","  pickle.dump(token_to_ix, open(token_file, \"wb\"))\n","  return token_to_ix, pretrained_emb\n","\n","def embed_text(x, max_len, token2ix):\n","  ques_ix = np.zeros(max_len, np.int64)\n","  x = clean_text(x).split()\n","  for ix, word in enumerate(x):\n","    if word in token2ix:\n","      ques_ix[ix] = token2ix[word]\n","    else:\n","      ques_ix[ix] = 1\n","    if ix + 1 == max_len:\n","      break\n","  return ques_ix\n","def tokenize(reviews):\n","  token2ix = {'PAD': 0, 'UNK': 1, 'SS' : 2,}\n","  for r in reviews:\n","    r = clean_text(r).split()\n","    for word in r:\n","      if word not in token2ix:\n","        token2ix[word] = len(token2ix)\n","  return token2ix\n","def category_from_output(output):\n","  top_n, top_i = output[0].topk(1)\n","  category_i = top_i[0].item()\n","  return category_i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2ddpTph3SEx"},"source":["class cloth_dataset(Dataset):\n","  def __init__(self, encodings, labels, metadata):\n","    self.embedded = np.array(encodings)\n","    self.label = np.array(labels)\n","    self.meta = np.array(metadata)\n","  def __getitem__(self, index):\n","    return self.embedded[index],\\\n","          self.label[index],\\\n","          self.meta[index]\n","  def __len__(self):\n","    # print(len(self.label))\n","    return len(self.label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hFnBIal4fh1","executionInfo":{"status":"ok","timestamp":1638816195064,"user_tz":300,"elapsed":2765,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"017eaea2-bc68-400d-b490-b8acfc4a3a21"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","EMBEDDING_DIM = 300\n","BATCH_SIZE = 8\n","LEARNING_RATE = 0.005\n","EPOCH = 11\n","data = pd.read_csv(os.path.join(data_dir, \"Womens_Clothing_E-Commerce_Reviews.csv\"))\n","data['Review Text'] = data['Review Text'].fillna(' ')\n","token2ix, pretrained_emb = get_glove_embedding(data['Review Text'],data_dir)\n","print(pretrained_emb.shape) # (len(vocab), embedding_dim)\n","\n","lengths = [len(x.split()) for x in data['Review Text']]\n","max_len = int(np.percentile(lengths,90))\n","data['embedded'] = data['Review Text'].apply(lambda x : embed_text(x,max_len,token2ix))\n","\n","meta_cols = ['Division Name', 'Department Name', 'Class Name']\n","def col2ix(x,col_cnt):\n","  if x in col_cnt:\n","    return col_cnt[x]\n","  return len(col_cnt.keys())\n","dummy_names = []\n","for c in meta_cols:\n","  col_cnt = {value:idx for idx,value in enumerate(list(set(data[c])))}\n","  data[c+'_'] = data[c].apply(lambda x: col2ix(x, col_cnt))\n","  dummies = pd.get_dummies(data[c+'_'], prefix=c.split()[0])\n","  names = list(dummies.columns)\n","  data = pd.concat((data,dummies),axis = 1)\n","  dummy_names += names\n","all_cols = dummy_names+['Age']"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","Loading saved embedding\n","(16335, 300)\n"]}]},{"cell_type":"code","metadata":{"id":"ZkmnwVr7kMiL"},"source":["\n","X_train_meta, X_dev_meta, y_train, y_dev = train_test_split(data[all_cols+['embedded']],\\\n","                                                                  data['Recommended IND'], test_size=0.2, random_state=42)\n","X_dev_meta, X_test_meta, y_dev, y_test = train_test_split(X_dev_meta,\\\n","                                                                  y_dev, test_size=0.5, random_state=42)\n","# print(X_train_meta.columns)\n","# X_train = [X_train[i] for i in range(len(X_train))]\n","from imblearn.over_sampling import RandomOverSampler\n","ros = RandomOverSampler(random_state=42)\n","X_train_meta, y_train = ros.fit_resample(X_train_meta, y_train)\n","\n","X_train, X_dev, X_test = np.array(X_train_meta['embedded']), X_dev_meta['embedded'], X_test_meta['embedded']\n","X_train_meta, X_dev_meta, X_test_meta = X_train_meta.drop(columns=['embedded']), X_dev_meta.drop(columns=['embedded']), X_test_meta.drop(columns=['embedded'])\n","\n","\n","# print('a',len(y_train), len(X_train), len(X_train_meta))\n","train_dataset = cloth_dataset(X_train, y_train, X_train_meta)\n","train_data_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","dev_dataset = cloth_dataset(X_dev, y_dev, X_dev_meta)\n","dev_data_iter = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_dataset = cloth_dataset(X_test, y_test, X_test_meta)\n","test_data_iter = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVbv67bm0A5e","executionInfo":{"status":"ok","timestamp":1638816199346,"user_tz":300,"elapsed":89,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"f274a8be-0f9e-452a-a6db-f9567d62d205"},"source":["from collections import Counter\n","Counter(y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 15467, 1: 15467})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"pvGQuDRL4Vp-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638816310291,"user_tz":300,"elapsed":375,"user":{"displayName":"Yixuan Meng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00008408025985696509"}},"outputId":"512f0433-2228-463a-9e0a-12a24ae7b306"},"source":["# 824-80x\n","class CNN_model(nn.Module):\n","    def __init__(self, token_size, pretrained_emb):\n","        super(CNN_model, self).__init__()\n","        dropout_rate = 0.8\n","        self.embedding = nn.Embedding(\n","            num_embeddings=token_size,\n","            embedding_dim=300\n","        )\n","        self.embedding.weight.data.copy_(torch.from_numpy(pretrained_emb))\n","\n","        hidden_size = 16\n","        k = 2\n","        print('hidden_size', hidden_size)\n","        self.conv1d = torch.nn.Conv1d(in_channels=300, out_channels=hidden_size, kernel_size=k)\n","        self.conv1d1 = torch.nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=k)\n","        self.conv_unit = nn.Sequential(self.conv1d, nn.ReLU(),nn.Dropout(dropout_rate),\n","                                       self.conv1d1, nn.ReLU(),\n","                                      #  nn.Dropout(0.5),\n","                                      #  self.conv1d1, nn.ReLU(),\n","                                       )\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.fc = nn.Linear(hidden_size+33, 1)\n","\n","    def forward(self, x, metadata):\n","      x = self.embedding(x) # print('0',x.shape)  # [bsz, len, 300]\n","      x = torch.transpose(x,1,2) # print('1',x.shape) # [bsz, 300, len]\n","      x = self.conv_unit(x) # print('2',x.shape) # [bsz, 128, len]\n","      out = torch.transpose(x, 1, 2) \n","      length = out.shape[1]\n","      embedding_v = out[:,-1,:] # print('3',embedding_v.shape) # [bsz, 128]\n","\n","      embedding_v = torch.concat((embedding_v,metadata), dim = 1)\n","      # print(metadata.shape)\n","\n","      logit = self.fc(embedding_v)\n","      # logit = self.fc_unit(embedding_v) # print('logit.shape',logit.shape) # [bsz, 1]\n","      return logit\n","    \n","net = CNN_model(len(token2ix), pretrained_emb).to(device)\n","criteon = nn.BCEWithLogitsLoss().to(device)\n","for batch_idx, (text, label, metadata) in enumerate(train_data_iter):\n","      text, label, metadata = text.to(device), label.to(device), metadata.to(device)\n","      output = net(text,metadata)\n","      output = output.squeeze(-1)\n","      # print(output.shape)\n","      # print(label.shape)\n","      loss = criteon(output,label.float())\n","      break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hidden_size 16\n"]}]},{"cell_type":"code","metadata":{"id":"pODOmaDB5mka"},"source":["def train(epoch, train_data_iter, dev_data_iter ,opt,criteon,net,device):\n","  def timeSince(since):\n","      now = time.time()\n","      s = now - since\n","      m = math.floor(s / 60)\n","      s -= m * 60\n","      return '%dm %ds' % (m, s)\n","  train_losses, dev_losses, dev_acc_list = [], [], []\n","  best_model, best_val_acc = None, float('-inf')\n","  cnt_step = 0\n","  current_loss = 0\n","  plot_every = 2\n","  dev_every = 2\n","  print('train len:',len(train_data_iter),'dev len:',len(dev_data_iter))\n","  print('learning_rate',LEARNING_RATE,'n_iters',epoch, 'optim','Adam','batch size ',BATCH_SIZE, 'lr_scheduler',None, 'device',device)\n","  start = time.time()\n","  for e in range(epoch): \n","    print('Epoch', e)\n","    net.train()\n","    for batch_idx, (text, label, metadata) in enumerate(train_data_iter):\n","      text, label, metadata = text.to(device), label.to(device), metadata.to(device)\n","      net.zero_grad()\n","      opt.zero_grad()\n","      output = net(text,metadata)\n","      # loss = criteon(output[0],label)\n","      output = output.squeeze(-1)\n","      loss = criteon(output,label.float())\n","      current_loss += loss\n","      cnt_step += 1\n","      loss.backward()\n","      opt.step()\n","    if e==0:\n","      print(time.time()-start)\n","    if e % plot_every == 0:\n","      tmp_loss = current_loss.item() / cnt_step\n","      train_losses.append(tmp_loss)\n","      current_loss, cnt_step = 0, 0\n","      print('%d %d%% (%s) loss: %.4f ' % (e, e / EPOCH * 100, timeSince(start), tmp_loss))\n","    if e % dev_every ==0:\n","      net.eval()\n","      eval_loss = 0\n","      y_pred, y_true = [], []\n","      cnt_eval_step = 0\n","      for batch_idx, (text, label, metadata) in enumerate(dev_data_iter):\n","        text, label, metadata = text.to(device), label.to(device), metadata.to(device)\n","        output = net(text,metadata)\n","        output = output.squeeze(-1)\n","        loss = criteon(output,label.float())\n","        # loss = criteon(output[0],label)\n","        result = torch.gt(torch.sigmoid(output),0.5).int() \n","        eval_loss += loss\n","        cnt_eval_step += 1\n","        y_pred += result.tolist()\n","        y_true += label.tolist()\n","      # print(cnt_eval_step, eval_loss, len(dev_data_iter))\n","      dev_losses.append(eval_loss.item()/cnt_eval_step)\n","      acc = accuracy_score(y_pred,y_true)\n","      dev_acc_list.append(acc)\n","      if acc>best_val_acc:\n","        best_val_acc = acc\n","        best_model = copy.deepcopy(net)\n","        torch.save(net.state_dict(), 'cloth_CNN.pth')\n","      print('%d %d%% (%s) loss:%.4f %s %s acc:%.4f' % (e, e / EPOCH * 100, timeSince(start), eval_loss.item()/cnt_eval_step, result.tolist()[:4], label.tolist()[:4], acc))\n","  print('best acc', best_val_acc)\n","  return train_losses, dev_losses, dev_acc_list, best_model,net # best_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhzT-IbB5sHF"},"source":["LEARNING_RATE = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKtd29tj76xe"},"source":["# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = CNN_model(len(token2ix), pretrained_emb).to(device)\n","# opt = optimizer.Adam(net.parameters(), lr=LEARNING_RATE)\n","opt = optimizer.SGD(net.parameters(), lr=LEARNING_RATE)\n","# criteon = nn.CrossEntropyLoss().to(device)\n","criteon = nn.BCEWithLogitsLoss().to(device)\n","train_losses, dev_losses, dev_acc_list, best_model, net = train(10,train_data_iter, dev_data_iter ,opt,criteon,net,device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkWm_y1YFCaW"},"source":["y_pred, y_true = [], []\n","for batch_idx, (text, label, metadata) in enumerate(dev_data_iter):\n","  text, label, metadata = text.to(device), label.to(device), metadata.to(device)\n","  output = best_model(text,metadata)\n","  output = output.squeeze(-1)\n","  loss = criteon(output,label.float())\n","  result = torch.gt(torch.sigmoid(output),0.5).int()\n","  y_pred += result.tolist()\n","  y_true += label.tolist()\n","acc = accuracy_score(y_pred,y_true)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kv6nGl8Mv4B7"},"source":["from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","def evaluate_p_r_f1_acc(y_pred, y_true):\n","  precision = precision_score(y_pred, y_true)\n","  recall = recall_score(y_pred, y_true)\n","  fscore = f1_score(y_pred, y_true)\n","  acc = accuracy_score(y_pred, y_true)\n","  return precision, recall, fscore, acc\n","def evaluate_cnn(model, data_iter):\n","  net.eval()\n","  y_pred, y_true = [], []\n","  for batch_idx, (text, label, metadata) in enumerate(data_iter):\n","    text, label, metadata = text.to(device), label.to(device), metadata.to(device)\n","    output = net(text,metadata)\n","    output = output.squeeze(-1)\n","    loss = criteon(output,label.float())\n","    result = torch.gt(torch.sigmoid(output),0.5).int()\n","    y_pred += result.tolist()\n","    y_true += label.tolist()\n","  p,r,fscore, acc = evaluate_p_r_f1_acc(y_pred, y_true)\n","  print('Precision: ',p, '\\tRecall: ',r,'\\tF-score: ',fscore,'\\tacc: ', acc)"],"execution_count":null,"outputs":[]}]}