{"nbformat":4,"nbformat_minor":0,"metadata":{"interpreter":{"hash":"db6b8d43935fb42c005c5bef54a6f402db4044fd2de36564b0cfd64a7717b6d2"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('siamese': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4,"colab":{"name":"cloth_bert.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"AjXNjL9YJ9zU"},"source":["data_dir = 'cloth'\n","output_dir = 'cloth'\n","from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, BertModel, AdamW, get_cosine_schedule_with_warmup\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import torch\n","model_name = 'bert-large-uncased'\n","bert_dir='bert-large-uncased'\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# DEVICE = \"cpu\"\n","max_len = 30 \n","BATCH_SIZE = 64\n","EPOCHS = 5\n","LEARNING_RATE = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoMXdDW5J9zX"},"source":["tokenizer = BertTokenizer.from_pretrained(bert_dir)\n","config = BertConfig.from_pretrained(bert_dir, num_labels=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtDBpy55J9zX"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler\n","import json,time\n","from  tqdm import tqdm\n","from sklearn.metrics import accuracy_score,classification_report\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"obrkyKeOJ9zY"},"source":["train_file = os.path.join(data_dir,'Womens_Clothing_E-Commerce_Reviews.csv')\n","data = pd.read_csv(train_file)\n","data['Review Text'] = data['Review Text'].fillna(' ')\n","\n","meta_cols = ['Division Name', 'Department Name', 'Class Name']\n","def col2ix(x,col_cnt):\n","  if x in col_cnt:\n","    return col_cnt[x]\n","  return len(col_cnt.keys())\n","dummy_names = []\n","for c in meta_cols:\n","  col_cnt = {value:idx for idx,value in enumerate(list(set(data[c])))}\n","  data[c+'_'] = data[c].apply(lambda x: col2ix(x, col_cnt))\n","  dummies = pd.get_dummies(data[c+'_'], prefix=c.split()[0])\n","  names = list(dummies.columns)\n","  data = pd.concat((data,dummies),axis = 1)\n","  dummy_names += names\n","all_cols = ['Review Text'] + ['Age'] + dummy_names\n","\n","# X_train, X_dev, y_train, y_dev = train_test_split(np.array(train[['Review Text','Age']]), np.array(train['Recommended IND']), test_size=0.2, random_state=42)\n","X_train, X_dev, y_train, y_dev = train_test_split(np.array(data[all_cols]), np.array(data['Recommended IND']), test_size=0.2, random_state=42)\n","X_dev, X_test, y_dev, y_test = train_test_split(X_dev, y_dev, test_size=0.5, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l18tN8KKJ9zY"},"source":["def get_dataloader(text, metad, label = None, test = False):\n","    input_ids,token_type_ids,attention_mask = [],[],[]\n","    metadata, labels = [], []\n","    for i,t in enumerate(text):\n","        encoded = tokenizer.encode_plus(text=t,max_length=max_len,padding='max_length',truncation=True)\n","        input_ids.append(encoded['input_ids'])\n","        token_type_ids.append(encoded['token_type_ids'])\n","        attention_mask.append(encoded['attention_mask'])\n","        # if not test:\n","        labels.append(int(label[i]))\n","        # else: labels.append(0)\n","        metadata.append(metad[i])\n","\n","    input_ids,token_type_ids,attention_mask = torch.tensor(input_ids),torch.tensor(token_type_ids),torch.tensor(attention_mask)\n","    metadata, labels = torch.tensor(metadata), torch.tensor(labels)\n","    data = TensorDataset(input_ids,token_type_ids,attention_mask,labels,metadata)\n","    loader = DataLoader(data,batch_size=BATCH_SIZE,shuffle=True) \n","    return loader\n","    \n","\n","train_loader = get_dataloader(X_train[:,0], X_train[:,1:].astype(np.int64), label = y_train)\n","dev_loader = get_dataloader(X_dev[:,0], X_dev[:,1:].astype(np.int64),y_dev)\n","test_loader = get_dataloader(X_test[:,0], X_test[:,1:].astype(np.int64),label=y_test, test = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sFhBNV_pJ9zZ","outputId":"66521edb-114a-4f9c-e6e2-660977eb67a6"},"source":["class Bert_Model(nn.Module):\n","    def __init__(self,bert_path,classes=2):\n","        super(Bert_Model,self).__init__()\n","        self.config = BertConfig.from_pretrained(bert_path)\n","        self.bert = BertModel.from_pretrained(bert_path)\n","        for param in self.bert.parameters():\n","            param.requires_grad=True\n","        self.fc = nn.Linear(self.config.hidden_size,classes)\n","    def forward(self,input_ids,token_type_ids,attention_mask):\n","        output = self.bert(input_ids,token_type_ids,attention_mask)\n","        logit = self.fc(output[1])\n","        return logit\n","\n","model = Bert_Model(bert_dir).to(DEVICE)\n","optimizer = AdamW(model.parameters(),lr=LEARNING_RATE,weight_decay=1e-4)\n","\n","schedule = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=len(train_loader),num_training_steps=EPOCHS*len(test_loader))"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /home/mengyixuan/Depression/liar/siamese/bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"S9WgFR82J9za"},"source":["from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","def evaluate_p_r_f1_acc(y_pred, y_true):\n","    precision = precision_score(y_pred, y_true)\n","    recall = recall_score(y_pred, y_true)\n","    fscore = f1_score(y_pred, y_true)\n","    acc = accuracy_score(y_pred, y_true)\n","    return precision, recall, fscore, acc\n","\n","def evaluate(model,data_loader,device,name='cloth_bert.pth'):\n","    model.eval()\n","    val_true,val_pred = [],[]\n","    with torch.no_grad():\n","        for idx,(ids,tpe,att,y,meta) in enumerate(data_loader):\n","            y_pred = model(ids.to(device),tpe.to(device),att.to(device))  # prob mat\n","            y_pred = torch.argmax(y_pred,dim=1).detach().cpu().numpy().tolist()\n","            val_pred += y_pred\n","            val_true += y.squeeze().cpu().numpy().tolist()\n","    p, r, fscore, acc = evaluate_p_r_f1_acc(val_pred, val_true)\n","    print('\\tF-score: ', fscore, '\\tacc: ', acc)\n","    return acc\n","\n","def train(model,train_loader,valid_loader,optimizer,schedule,device,epoch, name = 'cloth_bert.pth'):\n","    best_acc = 0.0\n","    criterion = nn.CrossEntropyLoss()  \n","    for i in range(epoch):\n","        start = time.time()\n","        model.train()\n","        print(\"### Epoch {} ###\".format(i+1))\n","        train_loss_sum = 0.0\n","        for idx,(ids,tpe,att,y,metadata) in enumerate(train_loader):\n","            ids,tpe,att,y,metadata = ids.to(device),tpe.to(device),att.to(device),y.to(device),metadata.to(device)\n","            if 'meta' in name:\n","                y_pred = model(ids,metadata,tpe,att) \n","            else:\n","                y_pred = model(ids,tpe,att) \n","            loss = criterion(y_pred,y) \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            schedule.step()\n","            train_loss_sum += loss.item()\n","            \n","            if(idx+1)%(len(train_loader)//5)==0:\n","                print(\"epoch {:04d}, step {:04d}/{:04d}, loss {:.4f}, time {:.4f}\".format(\n","                i+1,idx+1,len(train_loader),train_loss_sum/(idx+1),time.time()-start))\n","\n","        model.eval()\n","        acc = evaluate(model,valid_loader,device)  \n","        if acc > best_acc :\n","            best_acc = acc\n","            # torch.save(model.state_dict(),\"best_bert_model.pth\") \n","            torch.save(model.state_dict(), os.path.join(output_dir,name))\n","        print(\"current acc is {:.4f},best acc is {:.4f}\".format(acc,best_acc))\n","        print(\"time costed = {}s \\n\".format(round(time.time()-start,5)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WlNK5r5J9zb"},"source":["train(model,train_loader,dev_loader,optimizer,schedule,DEVICE,EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sx1mI4Y5J9zc","outputId":"ff7921c4-6985-4b0d-c0a8-463bd8bfde07"},"source":["model = Bert_Model(bert_dir).to(DEVICE)\n","model.load_state_dict(torch.load(\"cloth_bert-acc8838.pth\"))\n","acc = evaluate(model,dev_loader,DEVICE) \n","acc = evaluate(model,test_loader,DEVICE) "],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at /home/mengyixuan/Depression/liar/siamese/bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Precision:  0.9479381443298969 \tRecall:  0.9144704127299851 \tF-score:  0.9309035687167806 \tacc:  0.8837803320561941\n","Precision:  0.9402202412165706 \tRecall:  0.8920398009950249 \tF-score:  0.91549655348481 \tacc:  0.8590889740315028\n"]}]}]}